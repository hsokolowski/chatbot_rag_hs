# üß† RAG Chatbot ‚Äî PDF + Audio Knowledge Assistant

This project is a small end-to-end **Retrieval-Augmented Generation (RAG)** pipeline.
It loads PDFs and audio files, transcribes them, chunks and embeds the text,
stores the vectors in a local Chroma database, and uses either **Gemini** or **Ollama**
to answer questions directly from your documents.

---

## üì¶ What‚Äôs inside

| Component              | Description                                                           |
| ---------------------- | --------------------------------------------------------------------- |
| **Whisper**            | Converts audio/video files (mp3, wav, m4a, mp4) into text             |
| **PyPDFLoader**        | Extracts text from PDF files                                          |
| **LangChain + Chroma** | Handles vector embeddings, retrieval, and context search              |
| **Gemini / Ollama**    | LLM backend ‚Äî choose between local or cloud model                     |
| **RAG prompt**         | Forces the model to use only your data (no guessing or hallucinating) |

---

## ‚öôÔ∏è Setup

### 1Ô∏è‚É£ Clone & create environment

```bash
git clone https://github.com/yourname/chatbot_rag_hs.git
cd chatbot_rag_hs

python -m venv .venv
.venv\Scripts\activate   # Windows
# or
source .venv/bin/activate  # macOS/Linux
```

### 2Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

or manually:

```bash
pip install langchain langchain-community langchain-google-genai langchain-ollama
pip install chromadb sentence-transformers whisper-python python-dotenv
```

### 3Ô∏è‚É£ Install ffmpeg

Whisper needs it for audio processing:

* **Windows:** `winget install ffmpeg`
* **macOS:** `brew install ffmpeg`
* **Linux:** `sudo apt install ffmpeg`

---

## üóÇÔ∏è Project structure

```
.
‚îú‚îÄ‚îÄ data/                 # put PDFs, audio or text files here
‚îú‚îÄ‚îÄ chroma_db/            # generated vector store (auto-created)
‚îú‚îÄ‚îÄ main.py               # main script
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env                  # optional: store GEMINI_API_KEY here
‚îî‚îÄ‚îÄ test_questions.log    # output from test runs
```

---

## üöÄ Running

### ‚ñ∂Ô∏è Chat interactively

Ask questions based on your data:

```bash
python main.py --chat --llm ollama
```

or with Gemini (you can pass your API key inline):

```bash
python main.py --chat --llm gemini --api-key "YOUR_GEMINI_KEY"
```

### üß™ Run predefined test questions

This runs 3 fixed RAG-related questions and logs answers to `test_questions.log`:

```bash
python main.py --test --llm ollama
```

### üîÅ Rebuild the vector database

If you add new files to `/data` or want to re-embed everything:

```bash
python main.py --rebuild --llm ollama --test
```

---

## üí¨ Prompt behavior

The system uses a guardrail prompt that forces the model to answer only using
context retrieved from your files.

If the answer isn‚Äôt in the data, it responds with:

> ‚ÄúI don‚Äôt know based on the provided documents.‚Äù

This helps reduce hallucinations and ensures factual accuracy.

---

## üß≠ Tips to reduce hallucinations

* Use a **strict prompt** that forbids guessing.
* Lower the **temperature** (0.0‚Äì0.2).
* Combine **vector + keyword search (hybrid)** for stronger retrieval quality.
* Evaluate regularly (recall@k, precision).
* Keep embeddings updated when adding new data.

---

## ‚öôÔ∏è Command line options

| Flag                    | Description                                                                            | Example               |
| ----------------------- | -------------------------------------------------------------------------------------- | --------------------- |
| `--llm {gemini,ollama}` | Choose which model to use. Defaults to `gemini`.                                       | `--llm ollama`        |
| `--api-key <key>`       | Pass your Gemini API key directly via CLI. Overrides `.env`.                           | `--api-key sk-xxxxxx` |
| `--chat`                | Starts interactive chat mode.                                                          | `--chat --llm ollama` |
| `--test`                | Runs the predefined 3 test questions and saves results to a log file.                  | `--test`              |
| `--rebuild`             | Forces a rebuild of the Chroma vector database. Use when you add new files to `/data`. | `--rebuild --test`    |
| `-h` / `--help`         | Displays all available options and exits.                                              | `--help`              |

---

## üß† Example usages

#### 1Ô∏è‚É£ Run local Ollama chat

```bash
python main.py --chat --llm ollama
```

#### 2Ô∏è‚É£ Run Gemini chat (inline API key)

```bash
python main.py --chat --llm gemini --api-key "YOUR_GEMINI_API_KEY"
```

#### 3Ô∏è‚É£ Rebuild and run test questions

```bash
python main.py --rebuild --test --llm ollama
```

#### 4Ô∏è‚É£ Just rebuild vector DB

```bash
python main.py --rebuild
```
