 prepare homeworks we are assigning here on these two chapters. Of course we need it to have before that some sessions about large language models, about neural networks, about genAI introductions in general and fundamentals. So we can set at least have the same technology, understand the start understanding the paradigm and getting more familiar with the new concepts. So today's sessions will be splitting two parts. The first one is about productized and enterprise RUG. Actually we are mostly reviewing what the RUG platforms are and what are the available products that you can use. I will do also a quick recap on what actually RUG is. The second part of the section of the session will be on the architecture and design pattern. So after about 40 minutes we will see at the end of the first deck. You can switch some of you you can drop. Otherwise if you are interested you can still stay but just wanted to let you know that the second part of the session will be mostly focused on engineering aspects. We are discussing coding and architecture part. If you are interested of course you feel free to stay. Otherwise feel free to drop. Meanwhile my colleague Oxana is checking some settings in zoom. I think there is a limitation. I cannot share the entire screen and at some point I will need to switch from Google slides to show you some other screens. In case that doesn't work it will take a while to do the switch. Maybe we can fix that meanwhile Oxana and feel free to introduce me anytime in case if you find any other settings there which any flag there which was not set up already. Okay so I know you have the intro session with Max on what the retrieval augmented generation is. As I mentioned I will do a quick recap. Then we will do some review on the existing products and how you can leverage those products. So you can do your daily work not necessarily limiting to work but you can also use that in your own like private life. Then we will also discuss a bit about the pros and cons and limitations. So today's session the first part the the practitioner part is mostly a current and overview of the current products that we have available. So we can we can have the benefits of this new paradigm which is called RUG. First of all a quick recap in case if you missed the previous sessions in case maybe I don't know something was not clear. Just one thing a different perspective. I have to re-slide on what the right is. So first things first RUG basically it's just another software system that helps us crawl to some knowledge basis. How does it how does it work? It basically starts with an initial prompt. So imagine we are asking a question. Typically what we do or traditionally what we do with those questions is going to our database find some data there and then return answer to that question. But more traditionally the questions were addressed in a formal language which we call for instance APIs and those APIs have very clear specifications. The difference in the in the paradigm of genai that we can address these prompts, these questions in natural language. So in our mother tongue. But not only that we don't only go and retrieve something from the knowledge base which is relevant to our question. More than that we are initially augmenting that prompt. We are expanding it a bit with some knowledge which is private, usually it's private. It's related to our organization or to our private life. And then we go with that question to a large language model. And the large language model analyzes our question together with the data that we retrieved here from the knowledge source and returns us an answer because the magic the power of LLMs today is that they can answer in our mother tongue as well. So we don't need that much formal strict format of questions or answers. We don't need necessarily to pass specific arguments to questions to APIs. We don't need necessarily to use JSON files and then retrieve a specific XML format which we is further processed by another application. We don't need any of these. We just need our language to address the question to find something that's relevant there and augment our question. And then ask the LLM our initial question but in our own context. That's very important because our own context is something that normally the LLM doesn't have. So LLMs are just some neural networks. They are trained against a specific set of data. That dataset gives them the knowledge that they have. It cannot be altered. It's very hard actually to train an LLM. So they cannot know everything that we would potentially ask. We want to ask something about our current project, some Gira documentation. The LLM clearly doesn't have information about that. So we need to go here in the knowledge base and search that information attached to our prompt. And the job of the LLM is just to defraise it more nicely so that the answer will be also in in the same language that we addressed. We can even use Java for instance to ask questions. When we say language we are not limiting to natural languages but we are talking especially about natural languages because this is a novelty. A few years ago we couldn't do that. So why do we actually need Rugg? It's basically to expand the knowledge base of this LLM. And to expand actually the way we search the possibilities that we search through our knowledge base. Traditionally we make all it database. It's more or less the same thing. I believe you can you can look at the concept of knowledge base as a database but with some more relations, relations that are semantics. They are not only relations that are mathematically created between the entities in the database. The relations between entities into a knowledge base are very similar to what we have in our brain. They are semantics. So they are related to meaning. And the meaning has sense only is defined only for a specific language. Another reason we use Rugg is that it improves accuracy. So you may ask the LLM something but LLMs use a few sessions ago. You're playing with LLM Studio with different models. You have a homework. So you test different models. Some of you I saw your testing Gemma, other were testing Mistral. Then you are comparing this with Gemini or with GPT. So the differences are clearly there because these LLMs are bigger or smaller. They are trained against larger or smaller datasets in put datasets so they learn more or less. But depending on the quantity of the information and the quality of the data that they have initially, they would provide you better or worse answered. So that's why you need to improve the accuracy. And what does Rugg do? It's actually checking some additional context and feeds always in real time with fresh data, feeds the LLM. It's also very adaptable because you can change anytime some data in your Gira in your Confluence page documentation and then the Rugg mechanism will go and retrieve always. Only new data. So that makes the application that they are using Rugg to be efficient and versatile. So the main takeaway from here is that LLMs in general are limited to knowledge. To the knowledge that they were initially trained on, they know for them the reality is completely static. If they were trained against three terabytes of data that's all they know they cannot they maybe can statistically extrapolate but you don't have any guarantee that it's accurate. So that's why hallucinations occur. And also if you want to fine tune or D-trained this LLMs it's extremely expensive and is very slow. So as a worker around instead of doing that we are using the LLMs as they are. We better study their weaknesses, their limitations. And then when we talk to them we provide more context always so their task is reduced to only rephrasing some stuff not necessarily to providing knowledge. So by using Rugg we are already proposing a bit the role of the LLMs and we are transforming them into something that's more a very powerful translator if you like. So imagine you give them a question you also give them an answer because that's what you do from the knowledge base when you retrieve the context you are actually extracting the answer to that to your question. And the only thing you're asking the LLMs to do is hey you have the question you have the answer please rephrase this more nicely. If you know some additional data added there otherwise don't just rephrase it and give me the nice relevant answer in English whatever our language. So that's fundamentally what Rugg is. Now this is a key concept and I know you also discussed with even within preview sessions not only last one on the introduction but just for the maybe people for the audience that it's non-engineering. If you're not familiar to this embedding concepts I think it would be both interesting and fascinating for you to discover how they work because these embeddings are a key concept in generative AI and in retrieval augment to generation systems that are actually empowering us to find that relevant context. So when we ask a specific when we address a prompt then our system is able to find the most relevant answer to that one to that prompt within our knowledge base. Imagine you have a book and you have a quote you don't want to read the whole book right you are asking a question and I'm going back to this diagram you're asking this question your book is here in the knowledge source so instead of retrieving the whole book and attaching to the prompt and sending to the LLMs it's too large definitely the LLM would not be able to process it. Instead of doing that you are only extracting only what's relevant from the from the knowledge source. So how do we do that? We do that by finding something that is similar right now how do we define similar? This is very tricky concept but also fascinating. So here in the knowledge sources in order to discover what's the relevant context actually. First of all we are splitting. Imagine you have a book of 1000 pages we are splitting that book in page by page right and then all you need to do is to find what was the single most relevant or the top three most relevant pages within that book that that answer to your prompt. So how do you do that? But essentially you are using embeddings which are some numerical representations of the concepts right how does it work? In reality actually these embeddings are just are just another neural networks that were trained against a lot of content from that language and while they did this training and feel free to look to the preview sessions on neural network to understand how the training works. So while they were trained against that data they were establishing relations between different words and concepts. So in that case they know what's something like in a specific language what is closer to what word is closer to another word and what's further. Let's see how does it work? Basically embeddings are an numerical representation and the format it's a vector so that's why when we store them we say we are storing them into a vector databases and this numerical representation basically means taking each concept within that language and assigning them some numerical numerical values. You don't know what numerical values are assigned to them but by doing training by by allowing the neural network to see a lot of times those concepts eventually they will they will be able to to find to find the right number so that the neural network will tell what's the difference and how far it's a concept from another concept. Let's take an example of three words you have the concept of the mountain you have a concept of a river you have a concept of a canal so you know what what it means. So when we say assigning numerical assigning numbers to them you can start very very small by assigning a single number for instance we can call mountain one river two and canal three this is our convention and what does it do? It it basically help us make some differences between them. If we have only one single parameter here a single number it means we can use at least that that number for a specific scope what would be the scope. For now we the only thing we know about this concept is that they are different because one is different than two two is different than three right they are not the same thing definitely so we know mountain is not a river that's the only thing we know this technique it's called one heading coding maybe it's not that that relevant but then what happens when you start adding more numbers to these concepts so let's imagine we are looking to some characteristic of these concepts so for instance mountains or rivers or canals can be either natural or artificial care can be either static or mobile so if they are more natural we are we are assigning some numbers closer to minus one if they are more artificial we are assigning some numbers close to one if they are static we say it's minus one if they are mobile we say it's one same in that case it will actually reverse for the mobility in that case it would it would mean mountains are minus zero seven minus zero eight river are minus zero three zero seven right and canals are zero four zero five what did we do here it's we basically started describing this concept by choosing some numerical convention some numerical representation so basically we quantified how much a concept is natural or artificial or how much a concept is mobile or static if we would have another concept here such as car we will definitely say car is zero nine in terms of mobility right it's 90% most most of the time it's mobile right so that's that's something that defines this concept and when we start representing them it looks like this so mountain on on a on a plane it's here it's minus zero seven minus zero eight canals is here river is here and you clearly can see that angle between these two it's smaller so that what that does it mean for us is the river is closer it's more similar to a canal mountain it's actually almost opposite of a canal right so we know that this concept is definitely very very different than what it is and now we imagine coming back to our one thousand pages book imagine that book is talking about three concepts about the canal about the river and about the mountain and your question would be was the highest mountain in the world right so then the mechanism the algorithm what does it do it's looking through all the concepts in the book and it's choosing only the most relevant one mountain because it knows that our question it's not related to the other two concepts that what does the essence of of a retrieval augmented augmented generation mechanism and why why is that important is because we are able to isolate semantically what's relevant to our questions and we are ignoring the rest of the content like these two concepts so that we are not overwhelming elements with unnecessary information once we do that we have the prompt we have the concept we have the answer basically we pass it to our lm and the lm is answering by mostly refraising whatever we have there there are some challenges of course one of the most important challenges multilinguality because in different languages this concept this similarity might be different so when you have a cross-lingual understanding and now we search then you start facing challenges in identifying the relevant context and what does it mean it means that when you try to build a multilingual multi-lingual drag application it might not always work as you expect because it by designing lm are excelling in a primary language some of them which are very powerful such as gpt are working in most languages but the small language models which you've seen in lm studio for instance they are not performing that well in other languages so you need another lm maybe or you need to find a five different or crowns that's not a scope of the session it is just there's something that you need to be aware of so now let's see we can start reviewing some of the existing platforms and repositories and products on the internet because by using these drag systems you can go to your client and start basically leveraging them so you can create better product requirements documents analyze the requirements more or less faster or slower right slower not necessarily but it might be the case if you are not using the right choice and here is a pretty long list of some options you have on the internet I try to start with some options that are pretty much accessible without any specific license some of them might be licensed but it's still relevant because our scope is not necessarily to work with free options our scope is to understand what's possible and potentially pitch this in front of our clients or even investing in some tools that might help us personally right the first one we will review today it's a notebook lm some of you probably already heard about it on either a previous session it was mentioned definitely if not or maybe some of you will even have this notebook lm within a secret environment already or in client environment but if if not I can I can show you how it works so basically notebook lm is a it's a rag app which they call conventionally google calls it notebook in this notebook you can go and upload some sources you can go to google drive and pull some documents you can go and link it to some youtube videos or you can paste some text randomly I can show you an already one example that I was working with so here on the on the center on the main screen you have a summary of everything that was discussed that was sorry that was the address into the sources sources here are some files and I was using here some transcripts and some documents that we received from the client you see here is a summary of the discussion here is the some engagement strategy and next steps some notes right so this basically helps us loading as much documents actually it is a limit I think it's but it's I did not hit yet any limit it's like above 100 documents you can select all these sources here or you can disable them you can upload even more I'm using this for instance on even when I'm interviewing people especially if I have 10 candidates I interview the transcripts with all of them if I have some more client calls I'm looking to to the transcripts so I'm I'm make sure I never forget anything right so I loaded this here I cannot like show directly what's inside here because it was like a for a specific client maybe some confidential data but it's definitely the transcripts are here so this gives us some some some some summary I can ask you start addressing question give me some details about teen data let's say right so this teen data it's basically it's definitely something that an LLM knows right most of them such as Gemini are aware of what the concept means but what we are interested in is to make sure this is linked to our client most of the clients actually are leveraging this this concept whether if you call and they call it or not like this but this helps me give give answers in in context so basically it looks through all these documents here and help me understand what has been discussed already so this is very easy to review you've shown that it's possible to provide a Google Doc to notebook LM will it take the current version of this doc and use it forever or it will get a late reservation each time I ask question that's a good question me hi low as far as I know it is pulling the current version and then only sometimes but we don't have control on on notebook specifically but I think every few hours it's retrieving the new the new updates because it's caching the information it doesn't have time to go the runtime and query the document right I know from from trusted source that it's refreshing it from time to time but I don't know exactly the interval and there is another platform on top of what actually under notebook LM where you have more control but here this is just a ZZZ the product and you can use it as it is without having control on this then more than that you can it's not about about providing answers in context it's also about doing some other things such as let's create a mind map right so my map it's basically this tool going in asking more question more more more questions about our content and you to look like this so this definitely help you understand what were the threads discussed right and when you click on the thread then it's pulling you some data from the sources and it's giving you some references as well right you see here here is extract from the from the transcript so this is super powerful it's very easy to look through a lot of documents imagine you have here dozens of transcripts yeah let me let me address the other the other question when a dog provides updates based on new data does that mean LLM is being retrained or fine tuned or simply retrieving new context from the dog that's very relevant and good question mousadik thanks for asking it and LLM it's so for the LLM the world is static every time you go and ask a question it will return the same the same it will know the same thing it doesn't it's not being retrained or fine tuned every time you are discussing with it so every time you go to the LLM you need to provide again the context doesn't matter if you're just told to the LLM the context and your prompt if you go back with the prompt without providing the context it will simply forget everything if you go back and give another context it will take that context as is so its memory is zero is volatile right every time you're asking things it takes those for granted that that's its reality Gemini some gems do the same thing or similar thing as as notebook LM the difference here in gems is that you can basically upload some some basic instructions so you can instruct it on how you want to behave it works in notebook LLM as well but gems are a bit more oriented on creating a chatbot that responds to specific questions with a specific behavior I think they have to separate products from google actually developed in parallel mostly the same thing but with a different interface notebook LM has has the power to plug in with other other external sources that were introduced also to gem but later to gems but later notebook LM has also this fancy feature such as nine maps as you saw earlier gems basically and you you have the homework I am not insisting on it but these gems this gems oh I think they probably are not available anymore oh okay near here right english teacher so these gems are a bit more static they have some some some some instructions some descriptions and some specific static knowledge and that's it when you add a gem you can when you when you when you create a gem you can just add from file or from google drive but not necessarily from videos or from other stuff that notebook LM supports I use both of them actually it depends it's more comfortable for me to have a gem that I know it's it's always redeeming very well my english without re-instructing it again and again and the same thing it's available for custom GPT now I know probably not everyone here has custom GPs or chat GPT instance plus for instance I'm using clients GPT here and we have some custom GPT's which are very similar to gems you can go to my GPT you can configure that one you or you can create a new GPT and it works the same it's it accepts some configurable instructions and then some conversation starters and then you you upload some documents and it starts asking on uh uh answering on those documents this is important not not just for q&a it's important because it gives you the power to look through everything and basically bridge your knowledge to your scope let's let's say you need to write some codes but it's one thing to write the code blindly another thing is to write the code when your uh when you're uh um rug it's aware of the whole requirements now LM studio you tested it earlier on a previous uh on a previous stage in this in this program so LM studio allows you to load a model from here I prefer Jema models mostly if they are very um um light small very perform at the same time and very knowledgeable so you can uh you can use that and simply ask questions right what is the meaning of life or maybe what is the um owner of cyclone clearly doesn't know especially if the owner has has been changed uh in the past I don't know week right so what we can do is to go here a test dump file upload file and attach anything any PDF and then has the same question which will actually give you more information about um let's say I have this document which I'm calling it cyclone sorry you're still shooting them rounds of course ah okay okay one sec yeah uh LM studio okay okay so here in in LM studio you can ask was the owner of cyclone it knows what is the owner based on its own data like from 2023 probably you can upload a file such as cyclone let's let me find uh I have a note okay note pad so you can upload that one you have this uh in in your uh in your homework right and then in there in that document it was written that Alex is a Korean guy he's he's just something that I made up it's owning cyclone right it's a virtual name but basically it was able to retrieve data from here now you can configure that it's it's not only LM studio doesn't work only with uploading files you can go to settings here you can you can uh you can uh you can play with system prompts you can play with models the models that you want to choose right or you can go to developer mode and you can play also with uh with the with the context and the the way it's loading the the data so that it's actually also acting as a dog uh I think there is another said no actually no now it's a more limited but it's still working with your current um with your current stack locally and so it's it's it's helping you for privacy then further you have anything lm this is another product which is super powerful anything lm looks like this and let me let me share that screen now i think this one so anything llm basically is able to talk to your lm provider which is either holama or lm studio those that you you saw on the previous on the initial sessions so you can choose anything you can choose even open AI you can choose any any any model you like let's say i'm using lm studio the current one that is working is running i'm choosing the model here and then you have the option to choose your vector database in this vector database anything lm it's able to take your files and store them into into this vector database and this is much more powerful than you would have in um in a in a normal instance of lm studio because if the document is too too big it's it exceeds the context window of lm then lm studio will not be able to handle that but anything lm store this to a database so it doesn't have any limit basically here it's splitting the data in smaller parts and then it's identifying only the relevant one and when you ask a question that relevant part is going to to to the lm and it's answering your question you can choose even what embedder you need to use the embedder basically means the the model that gives you the embeddings those vectors these are other neural networks but that's all you need to know is there there are something similar to lm if you like but they are specialized in only finding similar concepts and many other many other functionalities i think anything lm it's probably the most complete tool i found which is free which can be used by both engineers and practitioners in building their own rags so here you can load physically unlimited unlimited amount of data and then start talking to that to that start creating a context and start talking sorry press the wrong button do you see my okay so you can create your workspace and then start talking to it i have here cyclim demo this is my workspace where i have some documents loaded it it it's work it it acts acts and work the same as gems and the same as notebook lm the difference is actually that everything is locally and it's leveraging a local model which is instantiating instantiated either using olama or using lm studio or any any provider okay you can ask anything of course it's providing you something from from lm studio if i'm going out with lm studio it's hard to share the screen the same same time but i can see on the lm studio logs that this is thinking and providing some answers here i can i can quickly go there and see let you see this so you see lm studio is answering to the question on what is the meaning of life and responding back to anything lm okay it takes a while for some reasons now it's coming so this is a big question blah blah blah but this empowers you actually to play with much more content feel free to download it on the internet i can send you some some references after the session and play with it it's very intuitive so you don't need too much training on it then uh next let me check here we have open web UI open web UI it's a browser based application which also has rags capabilities open web web UI it's basically doing the same thing as anything lm but it's not an application it's a web app so it can it can connect to jm i it can connect to any large language model you have you can even add more let's say llama and gpt4 and you can ask them you can ask them in parallel same thing and then you can compare the answers you can ask you you can add even more lm here i do not try more than four but it probably doesn't have a specific limit the reason why would we we would need a tool like this is that it works on web so we can we can instantiate that on our client side and it's pretty much prepared for enterprise grade it can go address and ask any lm further so you can use for your for your clients you can instantiate it there it's very intuitive because it looks like jgpt it has on the left side all these conversations then more than that it has a very nice admin panel which looks like this and within this admin panel you can go and set any model you want to it basically integrates all the models all the wide widely known models including gpt deep seek llama and so on you have different connections here you can leverage and it also has these documents feature which is a dog right so open AI itself it's able to chunk your data your files that you are uploading and using some specific instructions on how to chunk that you can even adapt and find in this after you upload your files there you you can choose how open web UI will process your input data and then you can start talking with them if you if you link this locally if you instantiate this locally on on your on your laptop and you connect it with a with an lm that comes from lm studio basically you have your own jiminai or your own your own jgpt fully locally but it's much more powerful than lm studio or than allama because it doesn't only provide an interface to an lm it provides an interface to more at the same time and it also has this radar functionality so it's more than talking to an lm it's actually talking to an lm using your your documentation as well this is very powerful feel free to try it and ask any questions if you if you have if you have any issues we are a bit over time I have two more products to review here one of them is glean so glean does the same thing as open a open web UI does or the same thing as custom gpt from open a i does but this is fully enterprise so you can we evaluate it for instance and we are discussing now with a with a client and adapting it what glean is able to do it's to plug in everything you have within your organization all the knowledge you have analyze your data do do the research even convert some graphics to code or the reverse it has an agent builder it has an agent orchestration unfortunately the demo is not for free you can only watch youtube videos on on glean but in general it does the same thing as open you open web UI or lm studio or anything lm does but it's productized so they basically instantiated on their site they are plugging in into their lm's they are paying for their lm's all you need to do is just go plug in your no data sources knowledge sources there and then feel free to explore the whole organization it's like they are providing notebook lm's for the whole company and the the information is of the share so you don't need always to update to upload your documentation there it's already there it's already indexed it works really quickly and anyone in the organization will not need to spend time on organizing notebooks all all data it's already there and similarly source graph source graph is another it's another it's another product that does similar things as notebook lm if you like or as custom gpt is I can show how it works we have it a list with taryx and with one client so source graph is able to index a lot of three repositories is basically the rug for code the same way as notebook lm or custom gpt works for with with documents with english source graph works for for coding notebook lm anything lm lm studio all these are not that great unless the the lm behind them are handling nicely the code part they are not excelling this part but source graph it's it's dedicated for this you can go to deep search for instance and ask anything about your codebase so for instance this is this is a question I addressed some time ago I wanted to track the changes that were made to a product after few iterations the prompt was pretty pretty large after 118 iterations so it did 118 steps looking through the whole codebase here are the steps it identified what what are what are the relevant documents or code parts or files with with the code right so identify what are the relevant files and then provided a comprehensive answer which looks like this so it it was able to summarize to look to millions of lines of code and summarize give give this give fast this this summary and then we can ask for that of course but we we were able to understand what has been changed regarding member profiles let's say and this is the code that is essentially doing the the major difference it even give us some diagrams so this is very extremely powerful if you use it in combination with an notebook lm or with a custom gpt you can you can basically achieve what I presented a few sessions ago on closing the loop within the software development lifecycle so you have all the documentation and all the code the holistic understanding of all of them so now you need of course to be aware that rags have some pros and cons pros are definitely what I mentioned earlier that they are reducing hallucinations they are giving more than just the lm does they are can be modular can be can be created in many ways like notebook lm on web within proprietary lm or with lm studio directly or with anything lm on top of some local lm like you can do it in many ways but they also have some some cons for instance the retrieval quality it really depends on the data if you throw there a lot of data which is contradictory it will start hallucinating it requires a lot of infrastructure such as vector databases indexing pipelines chunking pipelines and what does it mean it means sometimes it's very hard to maintain so that it means some clients simply choose not to use it and rather prefer to buy a product such as glean they also have limited windows so you are always relying on the context window of an lm the limitations that they have mostly they don't learn from the feedback directly right they don't learn from anything you need to build algorithm logic to do that because lm are not able to learn in real time if they if you provide conflicting or outdated sources it will give you answers according to the sources that you you provided and as a consideration the most important is rug it's probably the most the most relevant breakthrough that happened in 2023-2024 within our industry but now on top of rugs systems because now AI systems are able to handle more knowledge in this way using rugs they are going towards build toward evolving towards agenteic architectures and this is the topic of of our next module starting next week we are discussing about how to upgrade rugs and make them agenteic rugs most of the of the of the AI agents have some rug their foundation not necessarily all of them but the enterprise cases in most cases actually they are handling knowledge so some key takeaways before answering the question is make sure to to to make a difference between a rug and using lm's because lm's are not able to to learn and to understand more than they were trained initially with so rug helps in this regard with factual accuracy with real time data they are helping by reducing the hallucinations on of the lm's they don't need any retraining on the other hand they are a bit more slow than just talking simply to lm because it's it's just another man in the middle you are not asking the lm directly you are asking an only base first retrieving the prong the the context and then go to the lm that's why even charge pt it's not real time nothing doesn't work in real time they have limited reasoning and context awareness because no matter how you choose to split your documents and build a rug system the lm still have limited context awareness like they they cannot they cannot process more than a specific number of tokens or if you if you are to reduce the words let's say a few thousand hundred thousand words right doesn't dozens tens of thousands of words let's say maximum right so that's why you always need to to handle this gently and this comes with a lot of challenges now I think this is the end of the first of the first part of the first half of the session for the practitioners feel free to stay longer if you are interested in how to build actually if you want to build in house or ag like this we will we will look on some codes but first of all I would like also to answer to michaelo question or to Thomas as well so can source graph work with several repositories at the same time for instance for example our customer has many teams working on many services can we utilize it to get familiar with all the code across the organization and provide a source based on all the knowledge like architectural questions interactions and dependencies between services and the the authorities yes I can show you here we have 74 repositories 77 now from all all data sources such as github and bitbucket so that's super powerful because it's actually bridges different data sources I have a a YouTube recording on this topic and I will I will I will share this with you through to the group chat let me see if I can find it here yeah so is this one you probably do not see yeah you probably do not see it but I will share it on the group chat maybe it's the time stamp is not it's not actually the right one let me okay so yeah through graph it's able not only to look through all these code repositories but it's also able to look through across commits and across versions so you can compare an older version an older an older branch if you like of a repository with a new branch of another repository so that it's very helpful when when you want to migrate from I don't know from a technologist stack to another or when you want to migrate from some legacy logic to a modern logic it's very helpful when you need to build regression testing it's very helpful when you need to build to identify gaps between repositories when you need to understand how much of the code it's actually implementing everything you have in the requirements so yeah it's it's it's extremely powerful if you like we can take this separately we can we can discuss more about it definitely cannot that don't have time this session dive deep into specifically this question but there is a lot and the video I posted from from speakers cannot on youtube is very is relevant in this regard so much I was interacting with my gpt and out of curiosity I asked it what it knew about me and it's new about topics I was asking it oh yeah of course I told the lm learned about me through the conversation it just it is that just some other mechanism I was logged in in my account yeah of course actually if you go to custom to gpt here you have this personalization and you can you can enable customization you can then wait I think it's here yeah you can you can disable or remember disable or enable memory if you reference save memory is basically it's allowing it from time to save some summaries about your conversation and it will always get back to them if you have a record history if you enable all of this it basically going even across conversations and bring bring information about you and you will see all everything it knows about you you see here in saved memories so it's always using them I generally do not use this because when you analyze a lot of code you will give into a conversation some version of the code into other conversation other version of the code then it will start mixing them up so I'm disabled in everything related to memory there is also another account here setting where you can this is an enterprise account probably doesn't have this but in personal accounts you can add your own description you can say I'm too much I'm leaving there and I'm doing this and that so other than that it doesn't know anything about about you unless it was trained with some information about you unless you are very popular or unless it's searching on web of course but this is not at the LLM level it's the chat GPT application layer now thank you very much all for attending this first part of the session on products and how we can