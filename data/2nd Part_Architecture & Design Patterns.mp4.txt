 I believe it will still be interesting to stay if you have time. The next part is about how we actually do the architecture of this rug. And what are the design patterns that we are using. So in this section we are discussing about data chunking, about advanced retrieval techniques. Sorry, it's a duplicate here. Some reasons. Next, then about history management because the rug is actually also a mechanism to store history. Imagine having an knowledge base, but then you cannot or you don't necessarily want to go always to the knowledge base because some of the information is very new. It's just something you just discussed. The history actually it's just another knowledge base that the rug systems are leveraging. And then the whole kids how to build this context awareness. The context awareness is basically identifying the best way to identify your. The most similar part of the of the documents that you have stored in your knowledge base. Of course, coding part, but I will most of the times spend on this coding part and addressing these these three topics because you will see them how they actually are implemented. Just to have a quick overview of the concepts data chunking basically means. Not allowing the LM ingesting everything you have within your knowledge base. So what does it mean? It means you need to split it. You need to divide your your input into some smaller parts. And we have different techniques such as fix size variable chunking semantic chunking. Then after you are splitting it, you are saving this into some vector databases. And here is a list with comparison between them today for for the demo we'll use quadrant database. But it works pretty well with postgres. Postgres also has support for vector debits. Now more and more data bases actually added support for for vectors. So what they actually do is storing these vectors into into that into database there. And when you ask something it applying one of these formula and brings you the most relevant identified the most relevant vector. And a question from one of them do we know or can we control what chunking mechanism is using products you shared in the first part. Yeah, that's a great question and thanks for asking it in general know unless you choose. So let's go back to the to the to the previous list. In notebook, no chance you don't have any control on chunking mechanism. It's Google's job and in gems as well is Google's task to do the chunking they decided you actually don't know how they do it. It's a black box for us. This is good for some reasons on the other hand it doesn't provide of course autonomy and cannot it blocks you doing fine tuning and you will see it's a lot of fine tuning here. It's one of the most complex concepts in general actually the data chunking as well as the retrieving part. Custom GPT as well doesn't provide any support for for for for customizing the chunking. LMS Studio does some but it's only limited to this to this variable chunking or fixed chunking. So it doesn't offer semaphonic chunking capabilities and what does semaphonic chunking mean if you have this 1000 pages document you're not splitting it centers by sentence or page by page because you may you may split some some ideas. So semaphonic chunking helps you ending the idea and then continue with the next with the next with the next chunk. So LMS Studio doesn't doesn't help you in this regard also anything LMS doesn't allow you to do this. But it's still pretty powerful because you have a lot of customizations in terms of fixed and variable chunking and variable chunking might be actually powerful you can you can choose how many centers you want how long it's your your chunk right. And different different formulas actually there is an article here I published some time ago about this techniques you don't need necessarily to review it here it's it's pretty long. But I'll show you the variables for instance you can split it by sentence you can split it by by sentence by by using the naive approach you're looking to the limitators or by using different libraries which I'm actually using in in the next. In in the next email there are also this this kind of recursive chunking which. First of all looks at the other separators right but actually this doesn't I mean what you want to achieve here it's not only. Have a sweetable split by by separators but you also want to make sure you do not exceed some other some other stuff such as the total numbers of characters right. What you can do some specialized chunking such as when you do not handle sentences when you handle HTML files you are looking to drags you're looking to other stuff. So going back here anything I don't give you more than more than more than this open a web UI. That's the same thing so that's one of the main reason you need sometimes to build your own drag or your own mechanism. Because most of them are not dealing with semantic chunking semantic chunking is a very complex process and there are libraries that help you in this regard such as lama index which is probably the most powerful. Lama index has different methods to help you in maybe maybe do a mind X. Semantic chunking yeah semantic chunker so it gives you multiple ways actually to do the semantic chunk chunking because even this have some thresholds you can consider everything is similar to everything if you like or you can say everything is totally different than everything depends on. Simply how you define your understanding of the world so this has also its own complexity but lama index helps in helps in this regard pretty much. It has this break point percentile threshold it's a single parameter that you need to set up you don't need to do the whole logic on analyzing centers by sentence and checking if whether the sentence is related to the previous one or not. Open UI doesn't give you too much flexibility and semantic chunking. Actually nobody if you if you look closely to this but it gives you variable and fixed chunking which is still good for for smaller context. Glean doesn't give you anything it's just just a product you you you plug in everything there you don't have too much customization. But you have the benefit that is managed by by the providers so you don't need to to handle maintainers and so so you're graph as well so it doesn't doesn't give you anything related what vector store they use what embedding model they use anything like this. Now when we talk about retrieval part you will see that there are different ways you can retrieve data from from your knowledge base so first of all it's just do the just execute the query right start with your prompt pass it through to the embedding model and then identify we in the vector store what is the that chunk that is relevant to your prompt. And that gives you the context and that's sent to the LLM on the other hand you can do this much more in much more difficult way much more complex way there are different techniques such as. As vector store indexed your article indices hypothetical questions and so on context enrichment right I will show you actually all of these. And post retrieval you can do also some other stuff some such as re renting re ranking so let me give you a brief on what what they understand what they what they mean. So the economy may look actually like this when you when you do a search within your vector store you're not only retrieving that important chunk of data but you're actually looking forward and see where does that chunk come from what's the document that comes from. So if you don't have large documents for instance you have some some some confluence pages which are limited to four five ten pages let's say. By searching through them will not give you relevant answers because they may have many ideas many concepts there so that's why you need to split them by concept but then you still need to pass the whole context to the LLM. And that that gives you this. Where is the one sec okay auto margin retrieval I mean parent document right so you need basically to understand to check. What's the most relevant chunk and then retrieve the whole. All parent document in some cases it might work but there is no one size fits all scenarios you need to decide the time whether your document is too large does it succeed the context window do you want to do you want to do any truncated or do you want only to retrieve that that chunk of data and after you do the retrieval. You may want to see do you want to do reranking so reorganize them but reranking is not done with the same mechanism that helped you retrieve from the database is done with with another LLM so basically asking for the second opinion for the LLM do you think this is really relevant to my question yes or not. Or you can do query transformations so instead of asking a simple question you need to augment your question sometimes imagine the scenario when you are asking about secdom something tell me what secdom does what secdom is. And then after that you come back with another question and say now tell me what do they do and the LLM will will ask you not LLM actually the. The embedding model will ask you will will think what does they mean and it will look to the to the vector store and we do not find anything relevant or we find some everything is relevant today. They do not know you are talking about secdom like so that's why you need sometimes to transform your query every time you are asking the question to go and augment your question and look through the history. And when you look through the history then you understand that's actually talking about secdom and augment the current prompt without user being aware of that. Right. So let's actually see how that looks in practice initially I plan to switch the screen while I was talking on both slides and code but because it's not that easy to have them all on the same screen and I need to always release it. I showed you the slides and now I'm looking through the code. So let's imagine we let me give you a brief walk through to the code to the code structure right. So we have we have the API section here API basically. Yeah, I press the shortcut maybe that stopped me. Do you see my ID now? So now you have the API you have the API part is it's it's how I am I'm building the present points so we have chat we have chunk data we have add to quadrant this is the way we index and add to vector store we have the apps create and store it's not on the scope for today session but you can do a web scraping you have search quadrant so you'll see how the retrieval works. Then other than that you have the config file which has multiple parameters so I build this in a way that we can play with multiple multiple stuff here such as enabling disabled in the history management changing the size of the history. The history is basically an attached the an attached an attached. Knowledge base but I I prefer to use it and not only me like it is a practice to use it separately and then the knowledge base because knowledge base it's something very large where you are searching through it on some similar parts while history are passing most of. The questions and answers or you are passing some some last newest on that list some other configurations here not relevant at this point then we have the core part where we are actually building this app also not relevant to our context I'm choosing to use fast API and points. Then we have some helpers and I'll get back to this actually they're not relevant at this point but it's important for you to know that they exist we have integrations for some vector databases such as quadrant such as chroma phase we use for today's example quadrant. We also have integrations with different elements and I have here multiple providers such as Olama open AI just for convenience I'm using open AI. These providers are handled by using this element interface so element interface gives us a handler which allow me to switch through either this or that. So it has some lambdas that it's passing it's invoking the the the methods and passing the arguments. Further we have resources yeah we'll use this we'll see the how the chunking works how the text was chunked let's me remove this for now it's not relevant then we have some services and here in the services are actually our logic agent AI it's the core part the orchestrator of everything but I don't know that we have augment or we have retriever part we have a text text chunking logic. And some other stuff which are not relevant necessarily at this point and then we have some utils it's an environment check we don't need this as well. So let's say we go now and address the question. Screenshots have stopped okay so every time I'm pressing a alt p to to share the yeah interesting every time I'm I'm I mean enabling or disabling the presentation mode in Intelli J it triggers also some zoom shortcuts that's why. Okay let's say for now we want to do this chunking. So we have we have a postman instance here I hope you can you can see that right. So let's say we have a text seclude is an international blah blah blah what do we see here is that we have this I just copy this from from Wikipedia right so definitely not convenient because it's noisy to have this here. Is this repo open can we access it it's not it's not open publicly but it's open for cyclomers and definitely I can share that with you. Yeah so I definitely this is academic material you can you can you can use it to play with it more. But for especially for experience most experienced engineers here I think you're all able to understand the level of maturity of that code it's mostly POC's. Let's say we want to junk this. This text actually wait it stops the application it starts let me start it. Okay it takes about five seconds but we'll need to start. 10 seconds. How am I any success in allowing me to share the whole whole screen. What if I choose to windows can you now see both this in interlige. Yes cool interesting at least that is good. So I'm reshaling this and the and the chrome window. Okay let's see if the app started actually. I think I have some other heavy processes that are that are running let me close some of them. Anything LLM. Maybe LLM studio I can close it as well. Okay so now it started right we can go in in postman and ask. Ask the app to do the chunking so what we do it will do it will take this text. And we'll. We'll split it in some some smaller parts you see. So it didn't only split it in multiple parts. But actually it removed it removed the noise as well you see the this three eight nine ten whatever. Staff are here words split by. Actually they were included in the same chunk so. What happened here it's actually this chunks were augmented so we have now a rewritten. Version of it which is less noise less noise here and this is a good practice because sometimes if you are scraping the web especially you will get this noise. Let me show you how it worked. I hope it did not stop the screen if I'm. No we can see. You can see but you cannot see any more the interlige. Okay now can you can you see the interlige window yeah we can. Okay so what happened here it's. Last API. Okay so we received the in body the payload right. And then we took this text this input text. We pass it to the to the chunker. This is a chunker object which has a chunker chunk text mechanism. So chunk text basically takes this text. And it's checks if if I have. I have this method enable variable chunking implemented. If we look in the config file we will see that. Chunking has the enable variable. The flag turn false but semantic it's true. Right so you definitely just keep this part. But it will go to this semantic. And what it will do it will invoke the semantic chunking. Method with the text. And then. After we do this. We have our input text here. We are setting the threshold. So we are we are basically describing what what what do I want. To consider different if in into the embedding space. Two sentences are. Having a distance higher than 70%. Let's say. They are considered totally new concepts. So they are included in the different chunk. Right so I'm passing this this parameter to the. Splitter from Lama index. And I'm choosing the semantic model which is also. Configured get embedding model this one. You can see it here actually. Get embedding model what is that. No it's sorry it's it's coming from. Semantic model. Okay it's here so it's using open AI to do the embedding. So after we do this after we do the splitter we get some nodes. This is how Lama index call called on this this. So I'm getting the content of these nodes. Next time I will enable debugger actually but it just starts. Much more slower and we will lose time. We have the chunks now after we have the chunk basically we can come back to this to this method. Semantic chunking. I'm logging them right. But then we will we'll call the extent. The extent method what what does it do it takes all these chunks. And invoke the extent. Sorry. Oh actually it's adding to the chunks. No I thought it's it's another method that I I written to augment this. But it goes actually further. Here in case if we have in this chunk list if we have some duplicates it will be removed. And only then we will we will check if the augmented chunk flag it's it's enabled augmented chunk. It's here so if the augmented chunking is turned through then. Then this will call augment chunks. It's a static method from augmenter and it will actually what as you do it will return the identity chunk. Minitally return the same thing on one hand on the other hand it will return the enrich chunks and the summarize chunks. So this enrich chunks. We'll take a prompt. We'll pass my my chunks. And this prompts. To an LLM and we'll say you are a helpful assistant help me rewrite this and expand it for giving us more context. And after we have this where it we are doing the summarizing the same way. We are returning all these augment answers to the unique chunks. Maybe we maybe we could do again another another check for duplicates but it really is the case to add a limb to return the same thing. You can imagine the probability is almost zero. And then we are exporting the chunk to this reason file. Which is actually here in resources so we can see them here in the same manner as we saw in. Postman. Now the next step would be let's add them to cadrant so cadrant looks like this. It's here. It's our vector store currently doesn't have anything but but if I go to. If I go now to postman and call this add to cadrant method. Stop presenting mode without p. Let's see can I do this. I don't know without shortcut. Now we can look in the logs here. No it's still. Wow. It still stopped. So now we can look how this chunking works. This is exporting six chunks as well. It does the same thing but it's not only storing them. It's going to the vector store. After it invokes this summarizing and enrichment. Proms right it goes to the vector store and it's saving. And then it's saving to the to the JSON file and creating new cadrant collections which is called source text. So if you go here refresh it will have our. Our vector store is important to notice this 100. 1000 536. Is the size is the number of dimensions actually this this vector have. So. This is also configurable. It's here you see the vector size. And it's important because when you you do next time. Do the you can apply this. When you when you do the vector search it will the cost and similarity with you to lack against the same number of dimensions. Can we see any overlapping chunks you want to see or it's a question if it's if I put it is possible. I thought with chunk in there will be you didn't mention it but there is maybe often chunks like 50 characters to kind of keeping. It's whether we can see it's in the representation or is it something else. No I'm using for yeah I can show you. I'm using for the presentation sematic chunk because the most powerful and this is how our chunks look like cyclones international software. Right this is how it split by by ideas and this is how the enriched or the summarized chunks look like. Now if we go and ask a question actually what we can do is to go searching cadrant and say who own cyclone. It will retrieve you the most relevant chunks. It will be basically all of them. Who is the owner of the company called maybe. Yeah it will give all of them relevant but if I change threshold to 85. It will give me only three relevant chunks. Nothing you to be. I know them will specify that there is any owner. So if I go now to open a open web UI here I can even link. Maybe external or where is that sec. I can even link open web UI to my model and ask replace basically the postman with this who owns. Cyclone. And it will probably give something that it had it in within its weights. Okay nothing no name. But in addition to that what we can do is to add another phrase here and other sentence. Cyclone is a Japanese company and the nationality of this owner is Korean whatever called Alex. So I can go and add to cadrant this as well. Maybe I need to remove this. Okay so now if I'm rebuilding this. Maybe I can go back to cadrant. Clean up the collection. Maybe I can go actually go back to the app and instead of Augmented chunk I can I can disable it. We'll see an example of how the variable chunking looks like Thomas if you're asking about this. So now if I'm restarting the app it will not do the augmentation anyway of the chunks. It will simply give me. The split on the on the on the chunks that I am using here as example and I expect it will give me actually two chunks. One is the first part where describing Cyclone the second one is about the ownership of the company. So I'll give you the moment to start. Probably is because of zoom earlier it works. I mean the morning it worked much more faster. But I didn't have so many things open. Let me see if I can close some of the apps. But I cannot most of them are useful. Okay. Performance where is my? Yeah I have 29 out of 32 gigabytes of RAM full. Probably some residual stuff from Chrome. Yeah even IntelliJ it's 5 gigabytes. Anyway it started. So now if I'm going and I'm chunking this part it will not augment it anymore. It will add to cadrant it will index to cadrant this it only gives us two chunks. Let me see if I can. So one is it's exactly about the headquarters. Okay and the other one is about some general information about it where we have offices and about the Korean owner. Now if we go back to this and ask the same question. It will of course answer more aware of the of the new context because this has changed. So Ciclum is owned by Alex with Korean right. Pretty interesting. So now why does that change because here in cadrant we have different source. And we have different nodes. We can actually even see the graph the knowledge graph that dependency we have only two nodes but this is this is the dependency between them. And you can even see how how the vector look like. You see this is the default vector. So this is how actually the numerical representation of each sentence is stored. Imagine the example that I gave you had only two dimensions right. Only this one only the first one and the second one it was describing either if a concept is natural or artificial or the other and if it's static or mobile. Now you have 1000 dimensions 1536. So it describes a lot of other stuff but we don't know actually what what they describe because it's I'm using. Open AI text embedding add a zero zero to. But there is also the opportunity to use something like sentence transformals all mini L L L six blah blah blah this has one thousand twenty four dimensions. It's a bit smaller but still very powerful so you can go here and read. About these dimensions right what is what do you mean how it was trained this is not answers and it's also pretty powerful embedding model. Now if you if you're asking about other ways on how to do chunking let's say we are disabling the semantic part and we are enabled the the variable part. We have max two sentences max 120 words maybe let's say max 12 words and max characters 50 characters right. So if I'm restarting this instead of instead of testing each sentence if they if they are related within this text embedding add a zero zero to space. It will simply ignore that it will just look where the sentences are ending and starting. Okay started even faster and you know why that why is that is because I'm using a lazy loader for the embedding model. And the embedding model here it's it's it's an open AI but in fact it's actually loading other embedding models that are in the virtual environment. Anyway now if we if we go back to this. Wait I lost okay I'm removing everything from the collection again and I'm re-indexing this this will work much faster. Because it is just instead of for for chunks is just splitting them by by sentence by by variable whatever the dimensions I gave there so it will not have more than 12 words unless it exceed this characters. Let me show you how it how it works. So enable variable. The flag was here okay so the flag is here so it goes and do this variable chunking I hope you can see the code this way. Otherwise I need to read so it using this stunts to determine where the sentences it's ending. And then it has it it has a simple like traditional algorithm on checking whether the max word the max and the season mask characters are exceeded. And just. Take the longest the longest match. So the chunks will look like this. One two three four five six seven eight nine ten eleven twelve. Hmm I have a problem problem in the algorithm because it has more than 12 words that's interesting I never actually tested this. Yeah. It's probably because it actually allows to sentences and stands up consider to sentences like this anyway we have different chunking now clearly. But there is a risk because if you have large documents you may split ideas. Okay so let's see how the retrieval part works now. You can use in the same configuration and ask again who own cyclone. It will definitely find the relevant chunk let's see what is the relevant chunk. Yeah it found this so vector search retrieved two results. Cyclome is a Japanese company blah blah so it retrieved this one with the score 085 complexity similarity the other one cyclome is international software so it passed both these chunks. It alongside with with a prompt any yeah it is responded pretty accurately. Now let's walk through a bit to see how. How this is happening the retrieval part so when we go again to fast API. Okay when we go and address the question it's actually where is that it's it does search quadrant this is something that is invoked when we ask a question right but let's see how first how the search quadrant works. It takes the the body of course it looked through the text and the thresholds that were passing so remember. When I'm asking search quadrant I'm passing the thresholds similar to threshold 085. The limit how many chunks I want to return let's see I want only one it gives me only only the best one the best match if I am adding to if I'm adding five it will probably give me only two because only they are exceeding the threshold. So you have this and then you go to the vector store and search similar call this search similar using your text and threshold and so on. Search similar looks like this. It takes it takes a search it has a it's encoding it as a vector right and then it's addressing the question to the to the quadrant client it does this search which looks like I don't know if the package or what. But it works it will be removed in a feature okay it's marked removed. You have the query vector so basically you translate it the text into we can log this we order we can we can put a break break point here to see how the query vector looks like but it will be that 1000 something 500 numbers. And then this will return some results and we look on this results to make sure we want only those that are exceeding the threshold but then we will also we are also be sorting them right after we do this search similar we are returning it the result here when we if we do a deny if search but in chat actually things are working differently. So when when you go here instead of postman when you go here in the chat and ask the same question. Okay let's let's address actually the exact same question it gives us only one result. But in reality I'm not using I'm using 0.7 I guess in the application yeah so when I'm addressing this question. Is it the LJ okay you can see that we go with we go with the with the chat request whatever we have the user input we are. We are using this format response whatever agent respond so after we have the response we are we are. We are for Martin get for for another app actually not this open web UI open web UI has another end point but it was the same it calls the same stuff let me find it here yeah it's here the models chat completions. The completion yeah so it open web UI calls this which gives us the agent response method so the the prompt prompt text initially address is going to this respond. Now in the response I have the method handle conversation which take the user input handle conversation gives us an initial response after it it calls generate answer now this generate answer is is based on the user input but it's also based on some knowledge some knowledge that we are. We are we are building at the runtime always so we are calling this knowledge setup method the knowledge setup method has multiple losses first of all it is it is using a prompt to look through some sources and the source we are looking what what is the source that we want to look through into the configuration by default it's using the file. So the file actually looks like this I have a fallback config for the file for the agent knowledge it's a Jason file which has some some stuff some I don't know some random data actually. It doesn't have anything related to cyclone in case if the call to quadrant fails it goes to this it can go to any singles you can build any logic here it's like any traditional application actually. But in in a config file here I actually selected the source it's quadrant and the threshold is zero seven and the limit is three and then I don't want to do re rank I will show you how the re rank also works or I don't want to do alternative prompts. If I'm building here if I'm moving this to five for instance it will not only address my question and look through the knowledge base to find something similar but it will expand my LLM. It will expand my LLM to. To handle more prompts that are related to that. So this knowledge setup basically goes. Call the retriever method and the search. The search method goes to search similar vector store which you you you saw earlier. But then after you have the results. You check if the re rank is it enabled and if it's enabled you go here in the re rank chunk method and you are using another prompt you're using another LLM to verify if the below fragments are relevant and the return re order them. And after you are re ordering them actually you get the proper order of them and then you are cutting the the the response list to whatever you are truncating it to whatever you want like the top three most relevant after you did the retrieval from the vector store and the retrieval from the re ranking with the LLM. In case if that this doesn't work it will give you something from the from the Jason file. So after you do the the knowledge setup part you are generating really the answer and the answer basically is built on the knowledge. And on your user input here. But they both combined are actually passing through a prompt builder which is giving you something. Build from multiple multiple sub components so instead of passing to the LLM only your prompt you are adding some additional stuff such as behavior knowledge some history part. Right and then you have the build method which organizes them nicely but by being aware of the token budget. If you have some limitations here on the token budget. So after after you have this you have the messages basically you build the prompt but actually by building the prompt you are building some messages that we are sending to the LLM. So this generate message messages. Generate answer retrieve this messages which are the answers from from the LLM. And you have the initial response which you are returning directly. On the next on the next iteration of this session on the agentic part we will see how to expand this so we can do more than just providing the initial response we can enforce some guardrails and some other stuff. Right. But so far I believe we covered the way we are. Quaring and returning and retrieving answers and we were putting them into the right context so we can get answers like this. We are instantiating one instantiating an version of open web UI and we are pointing it to our model here. Right. So we can enable an example any model we want I also have all of my running locally I also have LM studio open web UI talking to all of them. And on on on on on the client side we are working this way because. You will see on the next session actually there are some other decisions that happen when you are asking a question we are checking if there are very confidential information or not. If they are we are replacing them with a local model and then we are passing that question. To a cloud model because it's much more powerful in terms of providing more more meaningful result. So by by that being said I think. The current session stops here. We will continue next part with expanding this regard architecture to an agentic one. I think the the next session on this topic expanding this topic is in it's in two weeks. But if you have any questions I still have 10 minutes and oxana I'm really sorry for this I thought we are and we have until 5pm. I was not aware that we actually finished a few minutes ago. But thank you for staying with me and for those that that already dropped. Definitely they can they can watch the recording. Any question? Let me see if there is anything I missed any question I missed. I have one in the chat so about the embedding vector. How do we select features? So you showed for example like natural or artificial mobile or not mobile or in the is it bound to specific embedding algorithm and it is always this feature. Or do we learn okay? It depends. Open a have their own closed model their own features you don't know which they are. You don't know how they are training them actually how they training them is not that it's not that. M embedding models. Hello. Yeah so embedding are kind of shallow these are neural networks which are not do not have more more. Hidden layers let me find a relevant diagram here if not I can prepare it for the next time. Wow such a long text with nothing. Yeah. So when when if you remember the last discussion on neural networks. Actually do not have it with me this time you had it with a storage I guess but you know there are some some multiple layers. So embeddings are actually the ways of a single layer and these these embeddings model are neural networks with they are called shallow. With a single hidden layer. They are actually embedding words so you can you can check for instance models like word to vac. You see this is the single layer this this this new architecture that are leverage within. Sentence transformer or open a idea are also transform architecture models. But the way basically the way they are building these features are dynamic and they are emerging based on the content that you are serving on the on the training phase. So there is no you can look on some papers for instance enter transformer the model that I showed you here which is open source. And you can see how they are choosing this how they are choosing this dimensions but for open iPhone instance you don't have them. And for each of these model the dimensions will be also different and that's why every time you are changing the model you will find different similarity scores. We can even do this exercise now. I'm not sure if it's not relevant anymore but if we still use exactly the same chunk. We will get different scores here in similarity. Right if we are changing the model. The problem is I'm changing the model I need to change also the the is not here. Chunking what is the chunking part. Ah here quadrant if I'm changing this model I need to change also the vector size I need to re index them as well. Yeah thanks. But you will definitely get different similarities. It's just it's very strange that we are discussing very much about the models and how they are great and which is bad the best but we are not discussing. The best embedding is the best embedding vector embedding model which is extremely important. It's absolutely important and you will see that even open AI have. Just release I think the last one is 3000 let me let me see open AI API. 3000 96 I guess for developers API pricing embeddings. Open none of our models. Embed embeddings come on not here are in. EPA platform. I'm not logged in okay. Although I thought I am actually M so models okay. Embeddings okay so this one text embedding three large. This has. I don't see you work. Ah you don't see that let me see if I can sure that one as well. Okay. Do you see now this open AI okay. Okay. So text embedding large three. It's 3772 us not I don't know why I remember something else but 3000. So these are much more higher than the higher the bigger they are here the size really matters the bigger they are. The most flexibility you have on defining what similarity means because this is really subjective even for two of us if two different persons you will ask them to classify two concepts give me. We can do this exercise we can based on the group chat right I can ask you now give me the similarity score between how how similar. I'm not sure. Percent percent in percentage how similar do you think a river and a canal is in your own language is it hundred percent 60% I would say it's 75% for instance right. Maybe someone else can give other other opinion so. This is the way we are perceiving the word the way we are actually deciding. How similar to concepts are it's actually describing our understanding of the world and the way we are mapping all the concepts into the hyperspace. And follow up question about do we calculate embeddings for a word or for the whole chunk. Absolutely for the whole chunk yeah that's the that's the thing because. How did started is started with the word to veck they were basically training against it's simple right you can brought for the language you have one thousand two hundred one hundred two thousand. One hundred twenty thousand words in a language if it's a big one English has maybe some word like one million words but you can brute for a seat easily. And then you can see the indifference sentences how often a word comes close to another word right so you can determine how similar they are or how opposite they are because of this relation. But then how do you brute for the whole all the all the potential combinations you cannot actually by using any any traditional LLM with a single layer. You are actually using another transformer architecture which is in fact. Another LLM if you like but it's it's just training instead of being instructed in responding into specific format as the LLM are. They are they are mapping this this concepts and the concept can be a group of words can be a super concept of concept a sub concept of multiple sub concepts of concept. It's it's actually super fascinating how this works. So basically this last question interview will be 10 no no problem. So this you know like in this oversimplified videos about how chat GPT works they are telling that it just gives you the word and then selects the next word like with the highest probability like something like this. But basically if embedding is done for the chance chat GPT is basically selecting the next channel. Now don't make don't don't don't make the confusion between how transform model transform model actually generates next token. So what it does I thought you were discussing this in the LLM session but we can address this next time as well. So for instance you have a transformers visual I think explainer this visual visualization OK. Long story short what does it how does it work? Come on. This has I have the visualization here. Yeah I have it. So this has basically some data input right which is which is splitting to tokens and it responds with some. It gives you some no I need another visualization actually transform or stay with me a bit. Visualization OK this one. So these are actually this transform and this is just GPT 3 GPT 2 maybe it's more relevant right so these transformers architecture are actually collections of the same screen yeah are just connections are just collections of neural networks here are some. Embedding one sex self attention MLP OK this MLP is a multi layer perceptron which is. A neural network and that new I can send you also YouTube video on that for the previous session. These are making predictions you are training them and they give you some predictions they give you based on an input what is the expected output right. If you have this. When you pass a word when you pass a sentence it this MLP will give you for for a specific part will give you some predictions right. And then the other one the other the other part will give you other prediction that's why this this networks are activated differently this transformer architecture is actually a collection of neural networks. And they provide different answers different estimations and they are all collected in this self attention mechanism which are actually some some matrices this matrices further. Have you see this attention head is basically collecting the outputs and if you put more attention heads. If you are doing an operation something like imagine the product of a matrix. If you are if you are aligning them you are predicting some tokens and then those tokens will will return to you to some probabilities. Now if you want to change the behavior of these probabilities you can adjust either the way you are training the whole the whole network so the whole weights and biases of them of the neural networks all neural networks. Or you can do some adjustments here so this matrix is so basically when you get some numbers here 0.7.4. So you can adjust them to 0.5. And this is what we call Laura low ranking. So then you can play with this alter them a bit. And retrieve some probabilities of the next token. And based on the one you choose let's say I want to choose the the one with lower probability it means you actually have another parameter which we call temperature it gives you more creativity because actually it allows one of the token. Not to be necessarily the preferred one of the most the most relevant one for the LLM but if you choose here during a prediction during a sentence that is predicted if you choose a third word with the fourth probability then the next of the future sentence will be changed as well it will be impacted. Now. This is the way how and how how this transformer how the LLM to work but the embeddings is different because it happens in a rug even before you you are a step. Right before you put the input data visualization and powers to this is our input you are actually building this input by giving a prompt from an user and finding into a vector store some relevant context which are. Covering that prompt and only then you are you are reaching this step and then the LLM does the prediction for you does the prediction for each multi layer perceptron. Collect them into attention heads play with them approximate them if they want quantize them and then gives you the probabilities of the of the responses and then you choose one of them of the token and then goes next and next in the loop. So, but this probability is for next words they are affected by where I am in this embedding vector space. Is it a space as a no. Okay. Actually not because the embedding vector space is just to find to find to find you to match you the. Or you are talking actually about this embedding because they are using the initial step so as I understand like to. Okay, I'm taking too much time but sorry guys or 17 people who are still here so again we have this initial question like I mean what is the meaning of life. Yeah we calculate the embedding then we find some spot in the embedding space like with some similar I mean context or answers yeah and then we proceed to actually this MLP layer which just. Select the next world probability but to the context of this embedding space spot that we are in. Yeah, but they are to independent infrastructure items here is the LLM here is your vector store right you are asking the question you go with a query find something relevant in a vector store. It can be any text you can even skip this step you will still have some context here which you are sending the prompt plus context. It's still there it's either you are bringing it from here or it doesn't matter for LLM it doesn't know it's the black box what happens before this step. So at this point you will have just a text which the text looks like you are intelligent agent that knows that cyclone owner is Korean and the question is. And the next the next sentence who is the owner of cyclone that's what LLM receives and then based on what it receives it passing to the to each MLP MLP gives its predictions passing through those attention head which are. Some matrices and then this metices have some operations you can you can think of them like products of actually they are products of matrices and gives you the. The further is the response right so it basically a collection of responses from multiple neural networks. The way is transformed yeah. So basically embedding is generating this first part that telling you are somebody with something and then it just proceeds to yeah but then here are the and trans of the LLM you also have indeed an embedding. You need also to convert to encode your text into something which is a list of tokens right this is another embedding but it is independent to this one. So basically a wrapper of LLM. Thank you very much for all relevant questions I think even though some people dropped they they will still find the meaningful the questions and the discussion and might be able to follow up later on this. I will talk to I will work with with oxana to send you follow up. Takeaways and materials on the group and after the next two weeks you will also get some homework assignments that are related.