 As we are starting with a rock topic, today we will have a small intro and then move on to the databases for Genii. Our speaker for today's both session is Maxim and we will start with intro. As I said, Maxim, you're very welcome to start. Hello. We're happy to see you all today. I will start from organizational topics as well. It depends on how fast we will move, potentially we will have some small break between sessions, but let's see. So today mainly we will focus exactly on the way how LLM can utilize external information for providing value to the users that basically utilize some systems and we will focus on the rock from the different perspective. First our focus and first our presentation will be around general rock, why do we need the rock and some more practical details that potentially will help you to navigate in this huge and complex world where you can find a lot of the tools for the different purpose, a lot of the databases, but you don't know where to start and maybe where and how to progress. So let's start maybe a bit from the introduction. So I'm a max right now at Siklom. I'm global head of cloud platforms and AI director. So I'm already nine years within Siklom. A long time ago I came here as a middle JavaScript engineer and went through the different stages of promotions and I was so lucky to have experience with different SDLC processes and stages that's right now I can jump much deeper not only into engineering topics in the developed topics, cloud topics and in a product topics as well. So from the start of chat GPT era I jumped to the AI direction as a self learner and I was amazed with the some of the experience that I got within the LLM and right now my main interest is to make a AI system as much reliable as it possible. That's why maybe you can find some of the topics that I did from the previous academies around the prompt engineering. This is where I started to adopt in LLM and try it to make them more reliable. And today we will go through like for this presentation we will go through three first topics. So we will focus on the definition and benefits. What we have from the rock some of the main use cases so from the practical life limitations and we'll cover a bit what tools you can use to speed up this process. So in general when we are talking about AI adaptation landscape so basically we have our base model and we want to direct that base model in our for example like domain or our use case or our application. So we need to start guiding or providing or maybe tuning the LLM and you always go from the simplest way of the tuning of the LLM to more complex. In some of the like articles you can find that even prompt engineering this is some kind of like tuning of the LLM because you are changing potential like behavior or narrowing direction of the LLM. And normally when you try to adopt LLM you are starting from that. The next stage and this is where we will focus mainly today for both of the sessions. It's retrieval augmented generation approach. This is where you already have some pipeline that helps you to add more extent and extend the knowledge and understanding of the LLM. The next stage it's like about the fine tuning mainly when you already need to adopt model in some way or like provide some way of the behavior of the model and for sure you can bring more knowledge to the model and the last one the biggest one where you have a combination of RUG and space-equipped fine tuning. And RUG for us the main approach we need to have like some fresh data or our proprietary data because all of the LLM have cutoff data like with the tool use they can get access to the more fresh data from the web but sometimes it's better and easier to provide our own data or especially if it's like proprietary and close data that you have access only you have access and they are not publicly. And it's one of the like most effective and cost effective way basically to let's say fine tune the model. And if we are starting from what exactly is RUG so this is the pattern that helps us to add additional knowledge to our model. In a simple way we have this like basic RUG pipeline where we have like first stage where we edit our knowledge and vectorize to some of the vector DB and then when our user use our system together with LLM basically first request is given to the vector database it's doing like some similarities or choosing a few of the chunks or like documents from that vector DB based on how many we set up and then LLM preparing the final response to our user. Why it's matters I already like partially explained but in addition it's quite often if you want to have like better accuracy in their responses because with the usage of the RUG approach you can put like exact data for the different topic that you want to utilize within your system when when user interacts with that. It's quite adaptable because you can quite fast change or update or add new knowledge to that RUG database much more cost effective and it's in addition it's increase efficiency especially when you need to provide like big amount of your like proprietary let's say data or like knowledge because for sure with the current size of the context window quite often you can feed a huge portion of the information to the LLM like with all of the knowledge that you want to utilize as a part of the output than when LLM is working on that data but a lot of the researchers show them that's bigger context windows that you utilize or like bigger prompt you you provided to the LLM you are getting much worse result because LLM quite often missing the exact like needed information in the middle and in addition you just provide in some noise together with that data like some of the words or information that's even not connected to your basically use case of the database of the communication with LLM. When we are going to deep dive so in the RUG pipeline in general we have two main stages but like when we go a bit to the details quite often and especially this information is missing when you when you're reading about the like RUG pipeline it's data preparation for that RUG pipeline because mainly it's focused on like the way of the chunking about the database you can use and how you put the data but it's quite important to start from the preparation of your data because I've mentioned for example when we provide a huge amount of the data to the LLM one of the big problem that's together with the data providing like water or noise that's just can navigate LLM when it use that like prompt information or text in the wrong direction so that's why quite often preparation of the of your data is quite important step as well then you have the step where you like preprocessing of the data like making like chunking a process because you can you can't just put all your 100 page documents to the vector database because it's probably huge and then LLM basically will get all of that 100 page documents as a prompt from the from the vector database so we have like different approaches how to split the information that comment with the with the documents and when we are talking about the documents the the information that's coming as an input can be like in many different formats because we are we are focusing mainly on the text formats but it can be audio it can be video it can be like images and different types like even chalice creep or python code basis then we after after the step when we split our data to the smaller pieces we are converting that smaller pieces to the numeric value and then that numeric value basically ingest to the vector database and we have a step of the retrieval it's one already our user of our system basically make the request and our system based on that request making the similarity search from the vector database and based on the results of that similarity search it's choosing like top key chunks like normally you are setting that's between like five to ten top results that you're grabbing from that vector database and then lm decide what part of the information from the chunk it should pick up to fulfill the intent of the request of the user and provide the output so chunking and approach to the chunking is a quite important part of this all of the rock system because like context window of the lm lm 6 limited for most of the lm's right now for sure we have quite a huge one even within the last jemenai versions they mentioned it can the context window can grow up to two million tokens but still the problem with missing information in the middle still persist with huge with huge context that you are providing to the model to the models even right now the situation is improving and mainly labs are working quite heavily to improve the retrieval and fighting and processing of the whole of the information that you provide into the lm but still this is the problem of reliability so to say and within the chunks you have like challenges to get the proper balance and if you provide like two large chunks if you if you split your document in a two large pieces then you will have as a result like not the best retrieval or a lot of the noise or water that you're providing as a part of the request the lm or if you put in it's too small then you are losing basically the main context of that information because it can be split it in a different part and it can be the case that based on the similarity search you will not get the full context of the information and lm can provide not reliable response you hear just representation of like three let's say base and main chunking strategies so fixed size semantic recursive but for sure you can find a lot of different approaches in terms of chunking so fixed size it's when we split our information basically in the fixed size like for example 512 tokens and we we are having like overlap and overlap basically help us to potentially preserve some context from the previous chunk and it help us to increase the potential quality of the chunks that we will have it is the simplest approach and for sure it has each each of the almost each of the approach or I would say like each approach has some cons and for for this for this approach this is the the problem that if you always have a standard size it can be the case that your chunk will be break in a mid of the sentence or will not have again still the main context that you need to preserve for this chunk and as a result the quality of the retrieval stage will be much lower the semantic one here quite often some ML system utilized and it's trying to split all of your information by the meaning and it has a much higher like accuracy and in most cases much higher quality of the retrieval stage because we are preserving in most cases like context that is needed to be in that chunk and when we are doing the similarity search and top results LLM is getting it's getting the proper context for all of the chunks that we picked up the problem for sure it's like slower and more expensive in terms of like compute and depends on the amount of the data that we have but in general the situation in this direction with the more compute and slower is getting better from the year to year and later on when we will have the next session I will give you some perspective on how fast things can can be changed with all of that tools that we utilize in right now for the agentic system and in in this ecosystem in general a recursive approach it's still mainly like coding coding approach without the utilization any of ML or AI system it's splitting of the information by separators with this approach you basically split we're chunking based on the structure of our document so we can identify the paragraphs we can identify the boundaries of the document and like for sure it's a bit more complex than fixed size and still we have that problem with the context because paragraphs still do not represent the full meaning of your text or your information and this is how that's three different approaches looks like so when we have the fixed size so we have two close to equal chunks and we have overlap of the parts of the sentence that we have quite often in our chunks when we have the recursive so recursive just split it our text based on the sentences and semantic it's more focused on what the meaning we have in each of the parts of our text that we provided right right now like for the last time a lot of the new approaches evolve in terms of chunking you can for sure hear about like agentic chunking while we utilize in the LLM and LLM provide to us more meaningful approach to this splitting on the chunks our information some embedding chunks when we utilize like exact embedding model and based on the the on the meaning of the text it help us to split the information better as well but they are coming with additional more extensive compute cost time and still we don't have like 100% reliable system that is the best approach for this splitting chunking or like embedding of the information so you don't need to implement like all of that that we mentioned like approaches to the chunks so some of the top libraries that's already fully implemented at least like basic approaches to the chunking so basically the length chain has has all of the basic approaches not only what I showed earlier in addition then they have more complex chunking approaches so it's a quiet known library like basically application framework that helping to build the agentic workflows and in under the hood it has a lot of additional connectors, side libraries that's helping us to build that agentic system and LLM index from the start it mostly was focused on the rock specific pipelines it has quite good performance it's data centric and it's just have a bit different approach how they implement this chunking strategy but you can find even the same chunking strategy implementation in both libraries so you can look there try them and most probably you will get quite good results from best in different chunk approaches for exact your cases. In terms of where mainly rock is using for sure we can we can talk about like from the perspective of general systems and from the perspective of the use cases when we're talking about perspective from the use cases so custom support, job search, detection and you name it while we have quite often updated system updated information with quite often updated system and mainly that system like proprietary notes fully publicly available especially but when we are talking about exact systems where it's mainly utilized it's like for the AI chatbots like for example if you have some support chatbots you are putting information as a to the rug like Q&A information or some support documents for your clients, search and discovery so combination of the keyword and vector search for better meaning of your searches, different compilates as an example it can be even how many of current systems like for example when we are talking about like cursor or other they basically use rug for the index indexation of the code base and much faster than finding the exact places in your code base for coding and helping to you and when we need the long context reasoning it's quite often utilized because again the quality of LLAM outputs is going down with the amount of the data that you feed as a part of their request so splitting and finding the exact parts of the information that you are going to provide is much better approach. So in terms of like common challenges still chunking and context to window as we as I mentioned about the problem with the chunking like if you provide in quite large chunks then it's mean that that's a quite large chunk then you feed to the model so we have the problem with the additional not needed information in exact that request then we have like retrieval quality so it based on the exact system that you utilize I mean like vector database or like approach to the data and latency and costs so for sure if we add to any system that we are developing if we add at some additional layer way you need to retrieve data or like make some filtering it's coming with the cost of latency. Some do some don'ts for the RAC system so quite often it's better to start from the simple approach even like from the simple base later on we will go a bit deeper on that. Use the metadata filtering in addition to the chunks most of the system provide possibility to add additional information and metadata like for example some category topic that's then it can be the part of your like filtering mechanism. Combining the vector and keyword search so hybrid approach and about that we will talk in our next session. Monitor the retrieval quality in addition to our like ingestion and our test we should prepare some pipeline that will double check the quality of our retrieval system. I will wait with the domain specific question so basically evaluation on that exact system that we are trying to build. To build like more reliable system not rely solely on the vector search so in addition you still have like your standard searches like keyword and we will discuss about some of the other approaches that's providing much better results when they work together as a hybrid approach and we're ranking approach as well. One of the biggest problems within the RAC this is the security and access control so it's quite complex to implement this sometimes but already some of the systems exist and we can just utilize them. Overload of the LM context so working on the balance of your chunks it shouldn't be quite big and for sure shouldn't be quite small because based on that your user of your system will get the result that's expected. Continuous update and do not skip evaluation framework. And here are not the three main chunking strategies but the systems that's or like libraries that you can utilize like for the scenarios of learning the main suggestion it's like LM chain because you can find a lot of the videos in YouTube it's quite extensive community around the LM chain and learning of the LM chain and that possibilities so basically you can utilize HROMA potentially sentence transformers or LAMA and some of the like key reasons it's you can like learn fundamentals and from the perspective or of risk-free like that stack that's I provided for the for the learning it's like fully open sourcing you can run just on your laptop but for sure you can if you do not use any data that shouldn't be sent to the models like chat GPT and stuff like that you can for sure utilize open EI or like Gemini embeddings for the for the easier approach and like as an MVP stage quite often we need just to change potential like HROMA and add a bit and other embeddings and for the more enterprise systems it can be the case that LM chain is not enough because it's quite good with the agentec workflows but has the problem with some agentec collaborative systems and finally come and again like open EI and Gemini embeddings one of the top right now in terms of quality the fault configuration so as a start for 80% of your cases chunk size if you are using like fixed chunk in approach so chunk size 512 tokens is enough and golden middle for sure based on your research you can identify different size of the chunks that work in for your cases better 15% of the chunk overlap top-caratriol I would say like 35 sometimes 10 and embedding dimensions between 700 and 1500s should be enough this data mainly on the research of the Nvidia and it's kind of like golden middle middle data and why you can start when you are building your chunk your rock systems but then within the evaluation and testing you can find your exact the best size for the chunks and for your rock system just few takeaways so rock this is the system that helping us to provide to the LLM external knowledge at the query time chunking is quite critical so I will not repeat about the exact numbers hybrid search is quite often better than like only vector only search start or always simple not only for the learning when you are starting prototyping all or development you can start from simpler systems not utilizing from the start the enterprise grade database is about what we will talk later today and quite important part it's evaluation of your rock pipeline not only like building but systems that will help you to make the evaluation of the quality of your pipeline is quite important time for the questions to be honest we only have three questions so yeah I believe we have already answered that but anyway how do we decide the chunk size for documents how do we decide chunk size for document you're starting basically from that chunk size that's I provided when we have like 512 tokens and with the slide in window of 15 percentage and this is why you are starting and then based on your like further experiments quite often when we are talking about like all AI or like ML development system you always have part as a research and then you based on the results that you are getting you are playing with the numbers so based on what you get and what you reviewed or maybe you already built the evaluation pipeline based on the numbers from the evaluation pipeline you can then just change your parameters and see if you are getting better results and then based on that researches based on that evaluations you can get the best number in your case in terms of what is the size and what is the approach to the chunks should be. Thank you Maxneum we have other question from Tolani she said that she found that LLAMs struggle was analyzing survey data she had started putting the documents as PDF and found that it works better so the questions are which chunk in strategy would an LLAM use for survey in PDF format versus CSV and which do you recommend. We will talk about this in our next session. Okay. So give that question for the next session in case we will not answer. Got it and the next question is RAC is just querying additional info from the internet and puts it in the context window of your model before providing the answer. Is it right? Almost except of internet this is our like separate database where we store in our data so this is like the domain pipeline but for sure we can say that this approach that's going to the internet and getting additional data this is a RAC system as well like a RAC system because we augmenting additional information to our LLAM but the base when we are talking about the RAC system it's mainly that we have some separate place where we store in some information that we want our system to utilize as a part of the of the answer of our LLAM. Okay thank you and the last one. Can I put RAC to my newly developed GPT and it will be very online and up to date. Is it? It's my newly developed GPT. Yep. Maybe a person can unmute and just say a few words what does it mean like newly developed GPT? Yes so it's my question so I just want to understand do I get it right about RACs? So if I have some LLAM whatever GPT or whatever and I don't want to retrain it I just want a new information like from the new database so it means that I can add this new shiny layer like RAC and then it's like chroma or whatever and it varies the data from this chroma before into the context window and just basically without retrain I will get all the all the information. Yes. Yes. Yes. Then it's fully correct. In a simple understanding maybe I just may clarify one point. Yes. So I believe the question was asked by Vasili so Vasili used like your I think you are oversimplifying it so in the first place you have to get your data into chroma so you have to get your data split it into chunks embedded, put it into chroma and then yes adding chroma and some RAC library on top of your GPT will work but you will get only your data you will not get all the new data available on the internet for that you would need to integrate with some other tools like internet search. Yes. I got it. Yes. Thank you so much for your questions. Maybe you have other or we can move on to the next part of the session. I have one question which is about the beginning part. It's just above my last message. Because without the beginning you mentioned that RAC injects into the prompt but did he mean the modern instead because I was confusing it's in the chat. Okay let's go. When we have the user query just to retrain. First step in most RAC systems where we based on that query our system find out the top parts of our data that we put to that database like five potential pieces of the information and then together with the prompt and that information our system making the request to the LLM and then LLM together with that information that we provided from the RAC database and the prompt that's like user query provide the answer to the user. Okay. So this will happen in the back end because on the front end I wouldn't see that. But I think of prompt as someone who is probably non-technical I think of whoever I've entered into the chat box. Yeah. For me just prompt this is anything that we are providing to the LLM all of the information. But yes I understand like just to show as a simple example of the RAC system as well. So when we create some projects for example in the chat GPT the project files that we are adding basically that projects then our RAC system because chat GPT just split that information to the chunks and then utilize that information when we are asking questions in our chat. It's the simple representation of RAC system and you can find all of that like GPs until you are providing the expected like BFF or instruction when you just provide the the files that you want GPs to utilize. It will be in addition to RAC system. Yeah. Thank you.